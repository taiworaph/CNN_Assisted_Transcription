{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Russian Language Transcription Implementation Using CNN with Grid-Search on individual Alphabeths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Moving Window grid-search | Constant Moving Window grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing all the neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taiwoalabi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.models import model_from_json\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Getting word images(Russian words) stored in the HD and converting the images to individual characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUTFILE = \"/Volumes/TAIWO/Writen_Names/ToDelete2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Width \t Image Height\n",
      "628\t\t\t156\n"
     ]
    }
   ],
   "source": [
    "im = Image.open((INPUTFILE + \"50.png\"))\n",
    "imgwidth,imgheight = im.size\n",
    "\n",
    "print(\"Image Width\", \"\\t\", \"Image Height\")\n",
    "print(\"{}\\t\\t\\t{}\".format(imgwidth, imgheight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "im.show()\n",
    "img = im.resize((int(imgwidth),32))\n",
    "img.save(\"/Volumes/TAIWO/Writen_Names/ToDelete2/TaiwoTested5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.show()\n",
    "image = cv2.imread(\"/Volumes/TAIWO/Writen_Names/ToDelete2/TaiwoTested5.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the already trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model-Loaded\n"
     ]
    }
   ],
   "source": [
    "json_file = open('/Users/taiwoalabi/Desktop/Machine_Learning_Projects/model_Russian2.json', 'r')\n",
    "    \n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "model.load_weights(\"/Users/taiwoalabi/Desktop/Machine_Learning_Projects/model_Russian2.h5\")\n",
    "print(\"Model-Loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the expanding grid-search algotithm over the words and collating the most probable characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Russian_Alphabeths = [ \"А\",\"Б\",\"В\", \"Г\", \"Д\", \"Е\", \"Ё\", \"Ж\", \"З\", \"И\", \"Й\", \"К\", \"Л\", \"М\", \"Н\", \"О\", \"П\", \"Р\", \"С\", \"Т\", \"У\", \"Ф\", \"Х\", \"Ц\", \"Ч\", \"Ш\", \"Щ\", \"Ъ\", \"Ы\", \"Ь\",\"Э\", \"Ю\", \"Я\", \"Space\" ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DictReal = []\n",
    "NotStartWords = [\"Space\",\"Ъ\", \"Ы\", \"Ь\" ]\n",
    "###-- This line of code is to obtain the first Russian Character itself --###\n",
    "Next_Iter_Start = 0\n",
    "while Next_Iter_Start < (imgwidth-10):\n",
    "    Value = []\n",
    "    LocationPict = []\n",
    "    Probability = []\n",
    "    for ii in range(int(Next_Iter_Start), int(imgwidth), 2):\n",
    "        cropped = image[0:32, int(Next_Iter_Start):ii+1]\n",
    "\n",
    "        cv2.imwrite(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) + str(Next_Iter_Start) +\".png\",cropped)\n",
    "        im = Image.open((\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) + str(Next_Iter_Start) +\".png\"))\n",
    "        img = im.resize((32,32))\n",
    "        X4L = np.array(img)\n",
    "        X4L = X4L.astype('float32')\n",
    "        X7L = X4L/255\n",
    "        X8L = np.expand_dims(X7L, axis=0)\n",
    "        prediction = model.predict(X8L)\n",
    "        \n",
    "\n",
    "#         if (Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1] not in NotStartWords) & (Next_Iter_Start == 0):\n",
    "#             Value.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "#             LocationPict.append(ii+4)\n",
    "#             Probability.append(max(prediction[0]))\n",
    "#             print(\"This inner Loop\")\n",
    "#         else:\n",
    "        \n",
    "#        if Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1] != \"Space\":\n",
    "        Value.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "        LocationPict.append(ii+1)\n",
    "        Probability.append(max(prediction[0]))\n",
    "        \n",
    "        ####---Clear out the image - Save HD space\n",
    "        os.remove(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) + str(Next_Iter_Start) +\".png\")\n",
    "\n",
    "    #Next_Iter_Start = LocationPict[(Probability.index(max(Probability)))]\n",
    "    print(Next_Iter_Start)\n",
    "    DictReal.append(Value[(Probability.index(max(Probability)))])\n",
    "    AAA = Value[(Probability.index(max(Probability)))]\n",
    "    YY = LocationPict[Value.index(AAA)]\n",
    "    Next_Iter_Start = YY\n",
    "    print('\\n')\n",
    "    print(DictReal)\n",
    "    print('Probability:', Probability)\n",
    "    print(Value)\n",
    "    print('\\n')\n",
    "    print('LocationPict:', LocationPict)\n",
    "    print('Found YY- Yeepee:', YY)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the constant grid-search algotithm window size over the words and collating the most probable characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DictReal = []\n",
    "Hey = 0\n",
    "for ii in range(0, int(imgwidth), 20):\n",
    "    Dict1 = []\n",
    "    Dict2 = []\n",
    "    cropped = image[0:32, ii: ii+10]\n",
    "    \n",
    "    #--Another for loop for optimization of the Max Prediction and recall values using Grid-Search algorithm--#     \n",
    "            \n",
    "    \n",
    "    \n",
    "    cv2.imwrite(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\",cropped)\n",
    "    im = Image.open((\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\"))\n",
    "    img = im.resize((32,32))\n",
    "    X4L = np.array(img)\n",
    "    X4L = X4L.astype('float32')\n",
    "    X7L = X4L/255\n",
    "    X8L = np.expand_dims(X7L, axis=0)\n",
    "    prediction = model.predict(X8L)\n",
    "    \n",
    "    #print(\"This is Prediction for:\", ii, max(prediction[0]), \"\\n\")\n",
    "    Dict1.append(max(prediction[0]))\n",
    "    \n",
    "    #print(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "    Dict2.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "    ###--The prediction phase of the \n",
    "    \n",
    "    \n",
    "    if Hey != 0:\n",
    "        for iii in range(0, 30, 5):\n",
    "            cropped = image[0:32, ii: ii+10+iii]\n",
    "            cv2.imwrite(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\",cropped)\n",
    "            im = Image.open((\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\"))\n",
    "            img = im.resize((32,32))\n",
    "            X4L = np.array(img)\n",
    "            X4L = X4L.astype('float32')\n",
    "            X7L = X4L/255\n",
    "            X8L = np.expand_dims(X7L, axis=0)\n",
    "            prediction = model.predict(X8L)\n",
    "    \n",
    "            #print(\"This is Prediction for:\", ii, max(prediction[0]), \"\\n\")\n",
    "            Dict1.append(max(prediction[0]))\n",
    "\n",
    "            #print(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "            Dict2.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "            \n",
    "        \n",
    "        for iii in range(0, 30, 5):\n",
    "             if (ii-iii >0):\n",
    "                cropped = image[0:32, ii-iii: ii+10]\n",
    "                cv2.imwrite(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\",cropped)\n",
    "                im = Image.open((\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\"))\n",
    "                img = im.resize((32,32))\n",
    "                X4L = np.array(img)\n",
    "                X4L = X4L.astype('float32')\n",
    "                X7L = X4L/255\n",
    "                X8L = np.expand_dims(X7L, axis=0)\n",
    "                prediction = model.predict(X8L)\n",
    "\n",
    "                #print(\"This is Prediction for:\", ii, max(prediction[0]), \"\\n\")\n",
    "                Dict1.append(max(prediction[0]))\n",
    "\n",
    "                #print(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "                Dict2.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "            \n",
    "            \n",
    "        \n",
    "        for iii in range(0, 30, 5):\n",
    "            if (ii-iii >0):\n",
    "                cropped = image[0:32, ii-iii: ii+10+iii]\n",
    "                cv2.imwrite(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\",cropped)\n",
    "                im = Image.open((\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\"))\n",
    "                img = im.resize((32,32))\n",
    "                X4L = np.array(img)\n",
    "                X4L = X4L.astype('float32')\n",
    "                X7L = X4L/255\n",
    "                X8L = np.expand_dims(X7L, axis=0)\n",
    "                prediction = model.predict(X8L)\n",
    "\n",
    "                #print(\"This is Prediction for:\", ii, max(prediction[0]), \"\\n\")\n",
    "                Dict1.append(max(prediction[0]))\n",
    "\n",
    "                #print(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "                Dict2.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "            \n",
    "           \n",
    "#             for iii in range(0, 100, 50):\n",
    "#             cropped = image[0:32, ii+iii: ii+300]\n",
    "#             cv2.imwrite(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\",cropped)\n",
    "#             im = Image.open((\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\"))\n",
    "#             img = im.resize((32,32))\n",
    "#             X4L = np.array(img)\n",
    "#             X4L = X4L.astype('float32')\n",
    "#             X7L = X4L/255\n",
    "#             X8L = np.expand_dims(X7L, axis=0)\n",
    "#             prediction = model.predict(X8L)\n",
    "    \n",
    "#             #print(\"This is Prediction for:\", ii, max(prediction[0]), \"\\n\")\n",
    "#             Dict1.append(max(prediction[0]))\n",
    "\n",
    "#             #print(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "#             Dict2.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1]) \n",
    "            \n",
    "            \n",
    "            \n",
    "        for iii in range(0, 30, 5):\n",
    "            cropped = image[0:32, ii: ii+10]\n",
    "            cv2.imwrite(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\",cropped)\n",
    "            im = Image.open((\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\"))\n",
    "            img = im.resize((32,32))\n",
    "            X4L = np.array(img)\n",
    "            X4L = X4L.astype('float32')\n",
    "            X7L = X4L/255\n",
    "            X8L = np.expand_dims(X7L, axis=0)\n",
    "            prediction = model.predict(X8L)\n",
    "    \n",
    "            #print(\"This is Prediction for:\", ii, max(prediction[0]), \"\\n\")\n",
    "            Dict1.append(max(prediction[0]))\n",
    "\n",
    "            #print(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "            Dict2.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1]) \n",
    "            \n",
    "            \n",
    "#         for iii in range(0, 200, 50):\n",
    "#             cropped = image[0:32, ii+iii: ii+300-iii]\n",
    "#             cv2.imwrite(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\",cropped)\n",
    "#             im = Image.open((\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\"))\n",
    "#             img = im.resize((32,32))\n",
    "#             X4L = np.array(img)\n",
    "#             X4L = X4L.astype('float32')\n",
    "#             X7L = X4L/255\n",
    "#             X8L = np.expand_dims(X7L, axis=0)\n",
    "#             prediction = model.predict(X8L)\n",
    "    \n",
    "#             #print(\"This is Prediction for:\", ii, max(prediction[0]), \"\\n\")\n",
    "#             Dict1.append(max(prediction[0]))\n",
    "\n",
    "#             #print(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1])\n",
    "#             Dict2.append(Russian_Alphabeths[prediction[0].tolist().index(max(prediction[0]))-1]) \n",
    "            \n",
    "    \n",
    "    print(max(Dict1))\n",
    "    print(Dict1.index(max(Dict1)))\n",
    "    print(Dict2[Dict1.index(max(Dict1))-1])\n",
    "    #img.save(\"/Volumes/TAIWO/Writen_Names/ToDelete_2/\" + \"Taiwo\" + str(ii) +\".png\")\n",
    "    Hey+=1\n",
    "    DictReal.append(Dict2[Dict1.index(max(Dict1))-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
